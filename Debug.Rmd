---
title: "debug"
output: html_document
---



```{r,echo=F, eval=T,cache=T, message=F,warning=F}

real <- c(rep(1, 1000), rep(0, 10000))
real_value_reversed <- c(rep(1, 10000), rep(0, 1000))
predicted_value_A = c(rbeta(300, 12, 2), rbeta(700, 3, 4),rbeta(10000, 2, 3))
predicted_value_B = c(rbeta(1000, 4, 3), rbeta(10000, 2, 3))
predicted_value_C = c(rbeta(3000, 12, 2), rbeta(7000, 3, 4), rbeta(1000, 2, 3))
pred <- prediction(predicted_value_A, real)
# pred <- prediction(predicted_value_C, real_value_reversed)

# concerns for this implementation:
# 1. we didn't sort the entire points lists
# 2. minor mistake in situation2 of mcc-f1 curve where there is no sharp angle



perf <- performance(pred, measure = "mat", x.measure = "f")
# mcc
mcc <- attr(perf, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.nor <- (mcc + 1)/2
# f score
f <- attr(perf, "x.values")[[1]]   
threshold <- attr(perf, "alpha.values")[[1]]

# get rid of NaN values
mcc.nor_truncated <- mcc.nor[2: (length(mcc.nor)-1)]
f_truncated <- f[2: (length(mcc.nor)-1)]

# get the last f score as approximate for the cut off point
# f.cp = 2/(2+(length(real[real==0])/length(real[real==1])))
# f.cp <- max(f_truncated[f_truncated < f.cp], na.rm = T)
f.cp <- f_truncated[length(f_truncated)]
# get the largest f score
f.max = max(f_truncated, na.rm = T)


x_lower <- sort(f_truncated[c(1: (match(f.max, f_truncated)-1))], decreasing = F, na.last = F)

x_higher <- sort(f_truncated[c(2: match(f.max, f_truncated))], decreasing = F)

# find corresponding mcc.nor
y_higher.index <- match(x_higher, f_truncated)
y_higher <- mcc.nor_truncated[y_higher.index]
y_lower.index <- match(x_lower, f_truncated)
y_lower <- mcc.nor_truncated[y_lower.index]

# auc1
auc1 <- sum(0.5*(x_higher - x_lower) *(y_lower + y_higher), na.rm = T)

x1 <- sort(f_truncated[c(match(f.max, f_truncated):length(f_truncated))], decreasing = F)
x1_lower <- x1[c(1: length(x1)-1)]
x1_higher <- x1[c(2: length(x1))]
# y1_higher.index <- match(x1_higher, f_truncated[match(f.max, f_truncated):length(f_truncated)])
# x1_lower.index <- match(x1_lower, f_truncated[match(f.max, f_truncated):length(f_truncated)])
y1_higher.index <- match(f.max, f_truncated) -1 + match(x1_higher, f_truncated[c(match(f.max, f_truncated):length(f_truncated))])
y1_lower.index <- match(f.max, f_truncated) -1 + match(x1_lower, f_truncated[c(match(f.max, f_truncated):length(f_truncated))])

y1_higher <- mcc.nor_truncated[y1_higher.index]
y1_lower <- mcc.nor_truncated[y1_lower.index]
# auc2
auc2 <- sum(0.5*(x1_higher-x1_lower)*(y1_lower + y1_higher))

# auc of mcc-f1 
mcc_auc <- auc1 - f.cp*0.5 -auc2   # 0.5 is also an approximate
# smallest distance to (1, 1)
distance <- sqrt(((1-mcc.nor_truncated)^2+(1-f_truncated)^2))
min_distance <- min(distance, na.rm = T)
# mcc-f1 measure
mcc_measure <- sqrt(2)*mcc_auc*(sqrt(2)-min_distance)
# round the number to three decimal place
mcc_measure <- round(mcc_measure, 3)

mcc_measure
```

```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real_value = c(rep(1, 5000), rep(0, 5000))
# A and B have different distributions for real positives 
# classifier A:
predicted_value_A = c(rbeta(2000, 9, 2), rbeta(3000, 2, 4), rbeta(negative_num, 2, 4))
# classifier B:
predicted_value_B = c(rbeta(3000, 4, 3), rbeta(2000, 3, 4), rbeta(negative_num, 2, 4))
# perfect classifier
real = real_value
prediction = cbind(predicted_value_A, predicted_value_B)
model_name= c("type A", "type B")
metric_index = c(f_score_index, mcc.nor_truncated_index, mcc_measure_index)

num_of_classifiers = ncol(prediction)
  list_of_metrics = list()
  num = 1
  while (num <= num_of_classifiers){
    metrics <- classification_result(real, prediction[, num])
    list_of_metrics[[length(list_of_metrics) + 1]] <- metrics
    num = num + 1
  }
  
  x_value <- c()
  y_value <- c()
  auc_metric <- c()
  length_of_metrics <- c()
  num = 1
  while (num <= num_of_classifiers){
    x_value <- c(x_value, list_of_metrics[[num]][, metric_index[1]])
    y_value <- c(y_value, list_of_metrics[[num]][, metric_index[2]])
    auc_metric <- c(auc_metric, list_of_metrics[[num]][,metric_index[3]], recursive = T)
    length_of_metrics <- c(length_of_metrics, length(list_of_metrics[[num]][, metric_index[1]]))
    num = num + 1

  }
  df <- data.frame(X = x_value, Y= y_value, measure = auc_metric, model = rep(model_name, times = length_of_metrics))
  df$model_and_measure <- paste(df$model, df$measure)


  ggplot(df, aes(x=X, y=Y, group = model_and_measure, color = model_and_measure,ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle(paste(type_of_curve, " Curve for different classifiers on \ndata set with positive versus negative" , positive_vs_negative)) +
  theme(plot.title=element_text(hjust=0.5))+ labs(x=x_axis, y=y_axis)
    
```

