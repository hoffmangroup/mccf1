---
title: "debug"
output: html_document
---



```{r,echo=F, eval=T,cache=T, message=F,warning=F}

predicted_value_1 = c(rbeta(3000, 9, 2), rbeta(7000, 2, 4), rbeta(1000, 2, 4))
pred <- prediction(predicted_value_1, c(rep(1, 10000), rep(0, 1000)))
perf <- performance(pred, measure = "mat", x.measure = "f")
# mcc
mcc <- attr(perf, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.nor <- (mcc + 1)/2
# f score
f <- attr(perf, "x.values")[[1]]  
perf <- performance(pred, measure = "prec", x.measure = "rec")
# precision
precision <- attr(perf, "y.values")[[1]]
# recall
recall <- attr(perf, "x.values")[[1]]
f(2) <- (1+2^2)*precision*recall/((4*precision)+recall)
f(0.5) <- (1+0.5^2)*precision*recall/((0.25*precision)+recall)

# measure(metric) of mcc-f1: auc*(1-d)

# get the last f score
f.cp = 2/(2+(length(real_value[real_value==0])/length(real_value[real_value==1])))
f.cp <- max(f[f < f.cp], na.rm = T)
# get the largest f score
f.max = max(f, na.rm = T)

x_lower <- sort(f[c(1: (match(f.max, f)-1))], decreasing = F, na.last = F)
x_lower[1] <- 0
x_higher <- sort(f[c(1: match(f.max, f))], decreasing = F)

# find corresponding mcc.nor
y_higher.index <- match(x_higher, f)
y_higher <- mcc.nor[y_higher.index]
y_lower <- c(0.5,y_higher)
y_lower <- y_lower[-length(y_lower)]

# auc1
auc1 <- sum(0.5*(x_higher - x_lower)*(y_lower + y_higher), na.rm = T)

x1 <- sort(f[c(match(f.max, f):length(f))], decreasing = F)
x1_lower <- x1[c(1: length(x1)-1)]
x1_higher <- x1[c(2: length(x1))]
y1_higher.index <- match(f.max, f) -1 + match(x1_higher, f[c(match(f.max, f):length(f))])

y1_higher <- mcc.nor[y1_higher.index]
# auc2
auc2 <- sum((x1_higher-x1_lower)*y1_higher)

# auc of mcc-f1
mcc_auc <- auc1 - f.cp*0.5 -auc2
# smallest distance to (1, 1)
distance <- sqrt(((1-mcc.nor)^2+(1-f)^2))
min_distance <- min(distance, na.rm = T)
# mcc-f1 measure
mcc_measure <- sqrt(2)*mcc_auc*(sqrt(2)-min_distance)
mcc_measure <- format(round(mcc_measure, 3), nsmall = 3)

df <- data.frame(X = f, Y= mcc.nor, measure = mcc_measure, model = "type A")
  df$model_and_measure <- paste(df$model, df$measure)
  
  df_label = df %>% group_by(model) %>% summarise(label_value = model)
  
  ggplot(df, aes(x=X, y=Y, group = model_and_measure, color = model_and_measure,ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white") +
  theme(plot.title=element_text(hjust=0.5))+ geom_text(data = , aes(label = measure), hjust = 0.7, vjust = 1)
```

```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real_value = c(rep(1, 5000), rep(0, 5000))
# A and B have different distributions for real positives 
# classifier A:
predicted_value_A = c(rbeta(2000, 9, 2), rbeta(3000, 2, 4), rbeta(negative_num, 2, 4))
# classifier B:
predicted_value_B = c(rbeta(3000, 4, 3), rbeta(2000, 3, 4), rbeta(negative_num, 2, 4))
# perfect classifier
real = real_value
prediction = cbind(predicted_value_A, predicted_value_B)
model_name= c("type A", "type B")
metric_index = c(f_score_index, mcc.nor_index, mcc_measure_index)

num_of_classifiers = ncol(prediction)
  list_of_metrics = list()
  num = 1
  while (num <= num_of_classifiers){
    metrics <- classification_result(real, prediction[, num])
    list_of_metrics[[length(list_of_metrics) + 1]] <- metrics
    num = num + 1
  }
  
  x_value <- c()
  y_value <- c()
  auc_metric <- c()
  length_of_metrics <- c()
  num = 1
  while (num <= num_of_classifiers){
    x_value <- c(x_value, list_of_metrics[[num]][, metric_index[1]])
    y_value <- c(y_value, list_of_metrics[[num]][, metric_index[2]])
    auc_metric <- c(auc_metric, list_of_metrics[[num]][,metric_index[3]], recursive = T)
    length_of_metrics <- c(length_of_metrics, length(list_of_metrics[[num]][, metric_index[1]]))
    num = num + 1

  }
  df <- data.frame(X = x_value, Y= y_value, measure = auc_metric, model = rep(model_name, times = length_of_metrics))
  df$model_and_measure <- paste(df$model, df$measure)


  ggplot(df, aes(x=X, y=Y, group = model_and_measure, color = model_and_measure,ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle(paste(type_of_curve, " Curve for different classifiers on \ndata set with positive versus negative" , positive_vs_negative)) +
  theme(plot.title=element_text(hjust=0.5))+ labs(x=x_axis, y=y_axis)
    
```

