---
title: "Comparison between different curves with prostate cancer data"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

Data preparation
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
prc_data <- read.csv("/users/ccao/Documents/LabNotebook/RMarkDown/k-NN/prostate_cancer_DRE_exam_set.csv",stringsAsFactors = FALSE) 
prc_data$y <- as.factor(ifelse(prc_data$diagnosis_result=="B",1,0))

prc_xtrain_full <- rbind(prc_data[1:25, 3:10], prc_data[76:100, 3:10])
prc_xtrain_fv <- rbind(prc_data[1:25, 3:6], prc_data[76:100, 3:6])
prc_xtrain_tv <- rbind(prc_data[1:25, 3:5], prc_data[76:100, 3:5])
prc_ytrain <- c(prc_data[1:25, 11], prc_data[76:100, 11])

mean <- sapply(prc_xtrain_full[, 1:8], mean)
sd <- sapply(prc_xtrain_full[, 1:8], sd)

prc_xtrain_full <- scale(prc_xtrain_full, center = T, scale = T)
prc_xtrain_fv <- scale(prc_xtrain_fv, center = T, scale = T)
prc_xtrain_tv <- scale(prc_xtrain_tv, center = T, scale = T)

prc_xtest_full <- scale(prc_data[26:75, 3:10], center = mean, scale = sd)
prc_xtest_fv <- scale(prc_data[26:75, 3:6], center = mean[1:4], scale = sd[1:4])
prc_xtest_tv <- scale(prc_data[26:75, 3:5], center = mean[1:3], scale = sd[1:3])
prc_ytest <- prc_data[26:75, 11]
                         
```


\textcolor{blue}{Function--LASSO}
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
library(glmnet)
library(ROCR)
library(ggplot2)
metrics <- function (trainx, trainy, testx, testy)
{
  cv <- cv.glmnet(x = trainx, y = trainy, family = "binomial", alpha = 1, nfolds = 10, type.measure = "class")
# optimal lambda from 10-fold cross-validation by lasso
lamstd = cv$lambda.min

fit <- glmnet(x = trainx, y = trainy, family = "binomial", alpha = 1)

std_probtest <- predict(fit, testx, type="response", s=lamstd)
std_predresp <- prediction(std_probtest, testy)

# ROC curve
perf_std <- performance(std_predresp, measure = "tpr", x.measure = "fpr")
# TPR
tpr <- attr(perf_std, "y.values")[[1]]
# FPR
fpr <- attr(perf_std, "x.values")[[1]]
# PR curve
perf_std <- performance(std_predresp, measure = "prec", x.measure = "rec")
# precision
precision <- attr(perf_std, "y.values")[[1]]
# recall
recall <- attr(perf_std, "x.values")[[1]]
# mcc-f1
perf_std <- performance(std_predresp, measure = "mat", x.measure = "f")
# mcc
mcc <- attr(perf_std, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.nor <- (mcc + 1)/2
# f score
f <- attr(perf_std, "x.values")[[1]]   

output <-cbind.data.frame(tpr, fpr, precision, recall, mcc.nor, f)
  return(output)
}
```


ROC
===
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
metrics.full <- metrics(prc_xtrain_full, prc_ytrain, prc_xtest_full, prc_ytest)
metrics.fv <- metrics(prc_xtrain_fv, prc_ytrain, prc_xtest_fv, prc_ytest)
metrics.tv <- metrics(prc_xtrain_tv, prc_ytrain, prc_xtest_tv, prc_ytest)


df_comb.roc = data.frame(TPR = c(metrics.full[,1], metrics.fv[,1], metrics.tv[,1] ), FPR = c(metrics.full[,2], metrics.fv[,2], metrics.tv[,2]) , model = rep(c("FULL","FOUR_VAR", "THREE_VAR"), times = c(length(metrics.full[,1]), length(metrics.fv[,1]), length(metrics.tv[,1]))))

ggplot(df_comb.roc, aes(x=FPR, y=TPR, group=model, color = model, ymin=0, ymax=1, xmin=0, xmax=1)) + 
  geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle("ROC Curve for three different classifiers") + 
  theme(plot.title=element_text(hjust=0.5))
```

PR
===
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
df_comb.pr = data.frame(Precision = c(metrics.full[,3], metrics.tv[,3], metrics.fv[,3]), Recall = c(metrics.full[,4], metrics.tv[,4], metrics.fv[,4]) , model = rep(c("FULL", "THREE_VAR","FEW_VAR"), times = c(length(metrics.full[,3]), length(metrics.tv[,3]), length(metrics.fv[,3]))))

ggplot(df_comb.pr, aes(x=Precision, y=Recall, group=model, color = model, ymin=0, ymax=1, xmin=0, xmax=1)) + 
  geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle("PR Curve for three different classifiers") + 
  theme(plot.title=element_text(hjust=0.5))
```



\newpage
```{r,echo=F, eval=F,cache=T, message=F,warning=F}
library(glmnet)
library(ROCR)
library(ggplot2)

# full model classifier
cv.full <- cv.glmnet(x = prc_xtrain_fv, y = prc_ytrain, family = "binomial", alpha = 1, nfolds = 10, type.measure = "class")
# optimal lambda from 10-fold cross-validation by lasso
lamstd.full = cv.full$lambda.min

fit.full <- glmnet(x = prc_xtrain_fv, y = prc_ytrain, family = "binomial", alpha = 1)

std_probtest.full <- predict(fit.full, prc_xtest_fv, type="response", s=lamstd.full)
std_predresp.full <- prediction(std_probtest.full, prc_ytest)

# ROC curve
perf_std.full <- performance(std_predresp.full, measure = "tpr", x.measure = "fpr")
# TPR
tpr.full <- attr(perf_std.full, "y.values")[[1]]
# FPR
fpr.full <- attr(perf_std.full, "x.values")[[1]]
# PR curve
perf_std.full <- performance(std_predresp.full, measure = "prec", x.measure = "rec")
# precision
precision.full <- attr(perf_std.full, "y.values")[[1]]
# recall
recall.full <- attr(perf_std.full, "x.values")[[1]]
# mcc-f1
perf_std.full <- performance(std_predresp, measure = "mat", x.measure = "f")
# mcc
mcc.full <- attr(perf_std.full, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.full.nor <- (mcc.full + 1)/2
# f score
f.full <- attr(perf_std.full, "x.values")[[1]]   

pr.full <- data.frame(Recall=recall.full, Precision = precision.full, model = "FULL")
roc.full <- data.frame(FPR = fpr.full, TPR = tpr.full, model = "FULL")
mf.full <- data.frame(F_score = f.full, MCC = mcc.full.nor, model = "FULL")
mp.full <- data.frame(Precision = precision.full, MCC = mcc.full.nor, model = "FULL")
```