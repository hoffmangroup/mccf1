---
title: "Simulation_Study_of_ROC_and_PR_on_Balanced_Data"
output: html_document
---

```{r,echo=F, eval=T,cache=T, message=F,warning=F}
positive_num = 5000
negative_num = 5000
real_value = c(rep(1, positive_num), rep(0, negative_num))
# A and B have different distributions for real positives 
# classifier E:
predicted_value_E = c(rbeta(2200, 10, 2), rbeta(2800, 2, 4), rbeta(negative_num, 2, 4))
# classifier F:
predicted_value_F = c(rbeta(2500, 4, 3.5), rbeta(2500, 3, 4), rbeta(negative_num, 2, 4))
# perfect classifier
predicted_value_perfect = c(rep(1, 5000), rep(0, 5000))
# random classifier
predicted_value_random = c(runif(10000, 0, 1))
# C and D have different distributions for real negatives 
predicted_value_C = c(rbeta(positive_num, 8, 3), rbeta(2500, 1, 7), rbeta(2500,4, 4))
predicted_value_D = c(rbeta(positive_num, 8, 3), rbeta(2500,3.5,4.5), rbeta(2500,3, 5))


```

Function of Binary Classification
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
library(ROCR)
library(ggplot2)

classification_result <- function (real, prediction){
  
pred <- prediction(prediction, real)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

# FPR
fpr <- attr(perf, "x.values")[[1]]
# PR curve
perf <- performance(pred, measure = "prec", x.measure = "rec")
# precision
precision <- attr(perf, "y.values")[[1]]
# recall
recall <- attr(perf, "x.values")[[1]]
# mcc-f1
perf <- performance(pred, measure = "mat", x.measure = "f")
# mcc
mcc <- attr(perf, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.nor <- (mcc + 1)/2
# f score
f <- attr(perf, "x.values")[[1]]   
# auc
auc <- round(attr(performance(pred, "auc"), "y.values")[[1]], 2)


# measure(metric) of mcc-f1: auc*(1-d)

# get the last f score
f.cp = 2/(2+(length(real[real==0])/length(real[real==1])))
f.cp <- max(f[f < f.cp], na.rm = T)
# get the largest f score
f.max = max(f, na.rm = T)

x_lower <- sort(f[c(1: (match(f.max, f)-1))], decreasing = F, na.last = F)
x_lower[1] <- 0
x_higher <- sort(f[c(1: match(f.max, f))], decreasing = F)

# find corresponding mcc.nor
y_higher.index <- match(x_higher, f)
y_higher <- mcc.nor[y_higher.index]
y_lower <- c(0.5,y_higher)
y_lower <- y_lower[-length(y_lower)]

# auc1
auc1 <- sum(0.5*(x_higher - x_lower) *(y_lower + y_higher))

x1 <- sort(f[c(match(f.max, f):length(f))], decreasing = F)
x1_lower <- x1[c(1: length(x1)-1)]
x1_higher <- x1[c(2: length(x1))]
y1_higher.index <- match(f.max, f) -1 + match(x1_higher, f[c(match(f.max, f):length(f))])

y1_higher <- mcc.nor[y1_higher.index]
# auc2
auc2 <- sum((x1_higher-x1_lower)*y1_higher)

# auc of mcc-f1
mcc_auc <- auc1 - f.cp*0.5 -auc2
# smallest distance to (1, 1)
distance <- sqrt(((1-mcc.nor)^2+(1-f)^2))
min_distance <- min(distance, na.rm = T)
# mcc-f1 measure
mcc_measure <- sqrt(2)*mcc_auc*(sqrt(2)-min_distance)
# round the number to three decimal place
mcc_measure <- round(mcc_measure, 3)


output <-cbind.data.frame(fpr, precision, recall, mcc.nor, f, auc, mcc_measure)
  return(output)
}

fpr_index = 1
precision_index = 2
recall_index = 3
mcc.nor_index = 4
f_score_index = 5
auc_index = 6
mcc_measure_index = 7

```

Plot funtion
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
# real is the real value(vector); 
# prediction is the predictions of classifiers(dataset with multiple columns and each column stands for the prediction under a classifier)
# metric_index is a two-element vector. the first element is the index of the metric that should be x axis, and the second element is the index of the metric that should be on y axis
# model_name is a vector of the name of of the models
# type_of_curve is a string. e.g "ROC"
# x_axis is a string that is the label for x axis
# y_axis is a string that is the label for y axis
# positive_vs_negative is the proportion of positive cases versus negative cases
classification_plot <- function(real, prediction, metric_index, model_name, type_of_curve, x_axis, y_axis, positive_vs_negative){
  num_of_classifiers = ncol(prediction)
  list_of_metrics = list()
  num = 1
  while (num <= num_of_classifiers){
    metrics <- classification_result(real, prediction[, num])
    list_of_metrics[[length(list_of_metrics) + 1]] <- metrics
    num = num + 1
  }
  
  
  
  # we don't have auc metric
  if (length(metric_index) == 2){
  x_value <- c()
  y_value <- c()
  length_of_metrics <- c()
  num = 1
  while (num <= num_of_classifiers){
    x_value <- c(x_value, list_of_metrics[[num]][, metric_index[1]])
    y_value <- c(y_value, list_of_metrics[[num]][, metric_index[2]])
    length_of_metrics <- c(length_of_metrics, length(list_of_metrics[[num]][, metric_index[1]]))
    num = num + 1

  }
  df <- data.frame(X = x_value, Y= y_value, model = rep(model_name, times = length_of_metrics))


  ggplot(df, aes(x=X, y=Y, group = model, color = model, ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle(paste(type_of_curve, " Curve for different classifiers on \ndata set with positive versus negative" , positive_vs_negative)) +
  theme(plot.title=element_text(hjust=0.5))+ labs(x=x_axis, y=y_axis)

}


  # we have auc metric
 else if (length(metric_index) == 3){
  x_value <- c()
  y_value <- c()
  auc_metric <- c()
  length_of_metrics <- c()
  num = 1
  while (num <= num_of_classifiers){
    x_value <- c(x_value, list_of_metrics[[num]][, metric_index[1]])
    y_value <- c(y_value, list_of_metrics[[num]][, metric_index[2]])
    auc_metric <- c(auc_metric, list_of_metrics[[num]][,metric_index[3]])
    length_of_metrics <- c(length_of_metrics, length(list_of_metrics[[num]][, metric_index[1]]))
    num = num + 1

  }
  df <- data.frame(X = x_value, Y= y_value, measure = auc_metric, model = rep(model_name, times = length_of_metrics))
  df$model_and_measure <- paste(df$model, df$measure)


  ggplot(df, aes(x=X, y=Y, group = model_and_measure, color = model_and_measure,ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle(paste(type_of_curve, " Curve for different classifiers on \ndata set with positive versus negative" , positive_vs_negative)) +
  theme(plot.title=element_text(hjust=0.5))+ labs(x=x_axis, y=y_axis)
    
  }
}




```


```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real = real_value
prediction = cbind(predicted_value_E, predicted_value_F)
model_name= c("type E", "type F")

# ROC 
classification_plot(real, prediction, c(fpr_index, recall_index), model_name, "ROC", "FPR", "Recall","5000:5000")
# PR
classification_plot(real, prediction, c(recall_index, precision_index), model_name,  "PR", "Recall", "Precision","5000:5000")
# mcc-f1
classification_plot(real, prediction, c(f_score_index, mcc.nor_index, mcc_measure_index), model_name,  "mcc-f1", "F1", "MCC", "5000:5000")
# mcc-precision
classification_plot(real, prediction, c(precision_index, mcc.nor_index), model_name, "mcc-precision", "Precision", "MCC", "5000:5000")
# mcc-recall
classification_plot(real, prediction, c(recall_index, mcc.nor_index), model_name, "mcc-recall", "Recall", "MCC", "5000:5000")
```

```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real = real_value
prediction = cbind(predicted_value_C, predicted_value_D, predicted_value_perfect, predicted_value_random)
model_name= c("type C", "type D", "perfect", "random")

# ROC 
classification_plot(real, prediction, c(fpr_index, recall_index, auc_index), model_name, "ROC", "FPR", "Recall","5000:5000")
# PR
classification_plot(real, prediction, c(recall_index, precision_index), model_name,  "PR", "Recall", "Precision","5000:5000")
# mcc-f1
classification_plot(real, prediction, c(f_score_index, mcc.nor_index, mcc_measure_index), model_name,  "mcc-f1", "F1", "MCC", "5000:5000")
# mcc-precision
classification_plot(real, prediction, c(precision_index, mcc.nor_index), model_name, "mcc-precision", "Precision", "MCC", "5000:5000")
# mcc-recall
classification_plot(real, prediction, c(recall_index, mcc.nor_index), model_name, "mcc-recall", "Recall", "MCC", "5000:5000")
```

Similar AUC but different ROC and PR 
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real = c(rep(1, 1000), rep(0, 1000))
# 1, 2 and 3 have similar AUC
predicted_value_J = c(rbeta(2000, 10, 1.5), rbeta(3000, 2, 4), rbeta(negative_num, 2, 4))
predicted_value_K = c(rbeta(3000, 3.05, 3), rbeta(2000, 3, 4), rbeta(negative_num, 2, 4))
predicted_value_L = c(rbeta(3000, 3.05, 3), rbeta(2000, 3, 4), rbeta(2000, 1, 12), rbeta(3000, 4, 4.5))
predicted_value_J = c(rbeta(350, 15, 0.5), rbeta(650, 2.25, 4), rbeta(1000, 2, 4))
predicted_value_K = c(rbeta(400, 3.3, 3), rbeta(600, 3, 4), rbeta(1000, 2, 4))
predicted_value_L = c(rbeta(400, 3.3, 3), rbeta(600, 3, 4), rbeta(455, 1, 10), rbeta(545, 4, 4.5))
prediction = cbind(predicted_value_J, predicted_value_K, predicted_value_L)
model_name= c("type J", "type K", "type L")

# ROC 
classification_plot(real, prediction, c(fpr_index, recall_index, auc_index), model_name, "ROC", "FPR", "Recall","1000:1000")
# PR
classification_plot(real, prediction, c(recall_index, precision_index), model_name,  "PR", "Recall", "Precision","1000:1000")
# mcc-f1
classification_plot(real, prediction, c(f_score_index, mcc.nor_index, mcc_measure_index), model_name,  "mcc-f1", "F1", "MCC", "1000:1000")
```


