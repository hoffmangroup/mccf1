---
title: "Simulation Study of ROC and PR on Imbalanced Dataset"
output: html_document
---

Data Simulation
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
# real binary outcomes
real_value = c(rep(1, 1000), rep(0, 10000))
real_value_reversed = c(rep(0, 1000), rep(1, 10000))
# classifier A: good early retrieval performance
predicted_value_A = c(rbeta(300, 9, 2), rbeta(700, 2, 4), rbeta(10000, 2, 4))
# classifier B:
predicted_value_B = c(rbeta(600, 4, 3), rbeta(400, 3, 4), rbeta(10000, 2, 4))
df <- cbind.data.frame(real_value, predicted_value_A, predicted_value_B)
```

Function of Binary Classification
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
library(ROCR)
library(ggplot2)

classification_result <- function (real, prediction){
  
pred <- prediction(prediction, real)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
# TPR
tpr <- attr(perf, "y.values")[[1]]
# FPR
fpr <- attr(perf, "x.values")[[1]]
# PR curve
perf <- performance(pred, measure = "prec", x.measure = "rec")
# precision
precision <- attr(perf, "y.values")[[1]]
# recall
recall <- attr(perf, "x.values")[[1]]
# mcc-f1
perf <- performance(pred, measure = "mat", x.measure = "f")
# mcc
mcc <- attr(perf, "y.values")[[1]]
# normalised mcc: [-1, 1] to [0, 1]
mcc.nor <- (mcc + 1)/2
# f score
f <- attr(perf, "x.values")[[1]]   
# thresholds
thresholds <- attr(perf, "alpha.values")[[1]]
# auc
auc <- attr(performance(pred, "auc"), "y.values")[[1]]
# mcc_f1 metric
# get rid of NaN values
mcc.nor_truncated <- mcc.nor[2: (length(mcc.nor)-1)]
f_truncated <- f[2: (length(f)-1)]
# new metric based on distance
# distance_total_sum <- 0
# for (i in 1:length(mcc.nor_truncated)){
#   d <- sqrt((mcc.nor_truncated[i]-1)^2 + (f_truncated[i]-1)^2)
#   distance_total_sum <- distance_total_sum + d
# }
# metric <- 1 - (distance_total_sum/length(mcc.nor_truncated))/sqrt(2)

total_sum <- 0
num <- 100
c <- (max(mcc.nor_truncated)-min(mcc.nor_truncated))/num
for (i in 1:num){
  # find all the points with mcc between c and c*i
  pos1 <- which(mcc.nor_truncated >= min(mcc.nor_truncated)+(i-1)*c)
  pos2 <- which(mcc.nor_truncated <= min(mcc.nor_truncated)+i*c)
  pos <- c()
  for (m in pos1){
    if  (m %in% pos2){
      pos <- c(pos, m)
    }
  }
  sum <- 0
  for (j in pos){
    d <- sqrt((mcc.nor_truncated[j]-1)^2 + (f_truncated[j]-1)^2)
    sum <- sum + d
  }
  total_sum <- total_sum + sum/length(pos)
}



metric <- 1 - (total_sum/num)/sqrt(2)

# finding the best threshold (closest to (1,1))
distance = c()
for (i in (1:length(mcc.nor))){
  distance <- c(distance, sqrt((1-mcc.nor[i])^2 + (1-f[i])^2))
}

cut <- thresholds[match(min(distance, na.rm = T), distance)]

output <-cbind.data.frame(tpr, fpr, precision, recall, mcc.nor, f, auc, metric, cut)
  return(output)
}
tpr_index = 1
fpr_index = 2
precision_index = 3
recall_index = 4 
mcc.nor_index = 5 
f_score_index = 6
auc_index = 7
metric_index = 8
cut_indxt = 9
```

Plot funtion
```{r,echo=F, eval=T,cache=T, message=F,warning=F}
# real is the real value(vector); 
# prediction is the predictions of classifiers(dataset with multiple columns and each column stands for the prediction under a classifier)
# metric_index is a two-element vector. the first element is the index of the metric that should be x axis, and the second element is the index of the metric that should be on y axis
# model is a vector of the name of of the models
# type of curve is a string. e.g "ROC"

classification_plot <- function(real, prediction, metric_index, model_name, type_of_curve){
  num_of_classifiers = ncol(prediction)
  list_of_metrics = list()
  num = 1
  while (num <= num_of_classifiers){
    metrics <- classification_result(real, prediction[, num])
    list_of_metrics[[length(list_of_metrics) + 1]] <- metrics
    num = num + 1
  }
  
  x_value <- c()
  y_value <- c()
  length_of_metrics <- c()
  num = 1
  while (num <= num_of_classifiers){
    x_value <- c(x_value, list_of_metrics[[num]][, metric_index[1]])
    y_value <- c(y_value, list_of_metrics[[num]][, metric_index[2]])
    length_of_metrics <- c(length_of_metrics, length(list_of_metrics[[num]][, metric_index[1]]))
    num = num + 1
  }
  
  df <- data.frame(X = x_value, Y= y_value, model = rep(model_name, times = length_of_metrics))
  
  ggplot(df, aes(x=X, y=Y, group = model, color = model, ymin=0, ymax=1, xmin=0, xmax=1 )) + geom_point(size = 0.2, shape = 21, fill="white")+
  ggtitle(paste(type_of_curve, " Curve for three different classifiers")) + 
  theme(plot.title=element_text(hjust=0.5))
  
}



```


```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real = real_value
prediction = cbind(predicted_value_A, predicted_value_B)
model_name= c("type A", "type B")

# ROC 
classification_plot(real, prediction, c(fpr_index, tpr_index), model_name, "ROC")
# PR
classification_plot(real, prediction, c(recall_index, precision_index), model_name, "PR")
# mcc-f1
classification_plot(real, prediction, c(f_score_index, mcc.nor_index), model_name, "mcc-f1")

```

```{r,echo=F, eval=T,cache=T, message=F,warning=F}
real = real_value_reversed
prediction = cbind(predicted_value_A, predicted_value_B)
model_name= c("type A", "type B")

# ROC 
classification_plot(real, prediction, c(fpr_index, tpr_index), model_name, "ROC")
# PR
classification_plot(real, prediction, c(recall_index, precision_index), model_name, "PR")
classification_plot(real, prediction, c(f_score_index, mcc.nor_index), model_name, "mcc-f1")
```
